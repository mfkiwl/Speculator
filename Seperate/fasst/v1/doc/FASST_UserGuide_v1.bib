% This file was created with JabRef 2.7.
% Encoding: Cp1252

@INPROCEEDINGS{Cichocki2007,
  author = {A. Cichocki and A. H. Phan and R. Zdunek and L. Zhang},
  title = {Flexible Component Analysis for Sparse, Smooth, Nonnegative Coding
	or Representation},
  booktitle = {Neural Information Processing, 14th International Conference (ICONIP'07)},
  year = {2007},
  pages = {811-820},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/iconip/2007-1},
  ee = {http://dx.doi.org/10.1007/978-3-540-69158-7_84},
  owner = {ozerov},
  timestamp = {2010.10.22}
}

@INPROCEEDINGS{Parry2006,
  author = {R. M. Parry and I. A. Essa},
  title = {Estimating the Spatial Position of Spectral Components in Audio},
  booktitle = {ICA},
  year = {2006},
  pages = {666-673},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/ica/2006},
  ee = {http://dx.doi.org/10.1007/11679363_83},
  file = {Parry_and_Essa_2006.pdf:Parry_and_Essa_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Abdallah2004,
  author = {S.~A.~Abdallah and M.~D.~Plumbley},
  title = {Polyphonic transcription by nonnegative sparse coding of power spectra},
  booktitle = {Proc.~5th International Symposium Music Information Retrieval (ISMIR'04)},
  year = {2004},
  pages = {318--325},
  month = {Oct.},
  owner = {ozerov},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{Araki2010,
  author = {S. Araki and A. Ozerov and V. Gowreesunker and H. Sawada and F. Theis
	and G. Nolte and D. Lutter and N.Q.K. Duong},
  title = {The 2010 Signal Separation Evaluation Campaign {(SiSEC2010)}: - {A}udio
	source separation -},
  booktitle = {9th International Conference on Latent Variable Analysis and Signal
	Separation (LVA/ICA'10)},
  year = {2010},
  pages = {114--122},
  address = {Saint-Malo, France},
  month = {Sep.},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@ARTICLE{Arberet2010a,
  author = {Arberet, S. and Gribonval, R. and Bimbot, F.},
  title = {A Robust Method to Count and Locate Audio Sources in a Multichannel
	Underdetermined Mixture},
  journal = {IEEE Transactions on Signal Processing},
  year = {2010},
  volume = {58},
  pages = {121 -133},
  number = {1},
  month = {jan. },
  doi = {10.1109/TSP.2009.2030854},
  issn = {1053-587X},
  owner = {ozerov},
  timestamp = {2010.04.12}
}

@INPROCEEDINGS{Arberet2006,
  author = {S. Arberet and R. Gribonval and F. Bimbot},
  title = {A robust method to count and locate audio sources in a stereophonic
	linear instantaneous mixture},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'06)},
  year = {2006},
  pages = {536--543},
  file = {Arberet_et_al_2006.pdf:Arberet_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.10}
}

@INPROCEEDINGS{Arberet2010,
  author = {S. Arberet and A. Ozerov and N.Q.K. Duong and E. Vincent and R. Gribonval
	and F. Bimbot and P. Vandergheynst},
  title = {Nonnegative matrix factorization and spatial covariance model for
	under-determined reverberant audio source separation},
  booktitle = {10th Int. Conf. on Information Sciences, Signal Proc. and their applications
	(ISSPA'10)},
  year = {2010},
  pages = {1--4},
  owner = {ozerov},
  timestamp = {2010.04.06}
}

@INPROCEEDINGS{Arberet2009,
  author = {S. Arberet and A. Ozerov and R. Gribonval and F. Bimbot},
  title = {Blind Spectral-{GMM} Estimation for Underdetermined Instantaneous
	Audio Source Separation},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'09)},
  year = {2009},
  pages = {751-758},
  owner = {ozerov},
  timestamp = {2008.12.13}
}

@INPROCEEDINGS{Attias2003,
  author = {H. Attias},
  title = {New {EM} algorithms for source separation and deconvolution},
  booktitle = {Proc.~IEEE International Conference on Acoustics, Speech, and Signal
	Processing (ICASSP'03)},
  year = {2003},
  pages = {297--300},
  file = {Attias_2003.pdf:Attias_2003.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.08}
}

@ARTICLE{Attias1999,
  author = {H. Attias},
  title = {Independent factor analysis},
  journal = {Neural Computation},
  year = {1999},
  volume = {11},
  pages = {803--851},
  file = {Attias_1999.pdf:Attias_1999.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.10}
}

@ARTICLE{Barker2005,
  author = {J. P. Barker and M. P. Cooke and Daniel P. W. Ellis},
  title = {Decoding speech in the presence of other sources},
  journal = {Speech Communication},
  year = {2005},
  volume = {45},
  pages = {5-25},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/j.specom.2004.05.002},
  file = {Barker_et_al_2005.pdf:Barker_et_al_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2007.12.18}
}

@ARTICLE{Ben-Yishai2004,
  author = {Ben-Yishai, A. and Burshtein, D.},
  title = {A discriminative training algorithm for hidden {M}arkov models},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {2004},
  volume = {12},
  pages = {204--216},
  number = {3},
  file = {Ben-Yishai_and_Burshtein_2004.pdf:Ben-Yishai_and_Burshtein_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.06.10}
}

@INPROCEEDINGS{Benaroya2003a,
  author = {L. Benaroya and F. Bimbot},
  title = {Wiener based source separation with {HMM}/{GMM} using a single sensor},
  booktitle = {Intl. Conf. on Indep. Component Analysis and Blind Source Separation
	(ICA'03)},
  year = {2003},
  pages = {957--961.},
  address = {Nara, Japan},
  month = {Apr.},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@ARTICLE{Benaroya2006,
  author = {Benaroya, L. and Bimbot, F. and Gribonval, R.},
  title = {Audio source separation with a single sensor},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2006},
  volume = {14},
  pages = {191--199},
  number = {1},
  abstract = {In this paper, we address the problem of audio source separation with
	one single sensor, using a statistical model of the sources. The
	approach is based on a learning step from samples of each source
	separately, during which we train Gaussian scaled mixture models
	(GSMM). During the separation step, we derive maximum a posteriori
	(MAP) and/or posterior mean (PM) estimates of the sources, given
	the observed audio mixture (Bayesian framework). From the experimental
	point of view, we test and evaluate the method on real audio examples.},
  file = {Benaroya_et_al_2006.pdf:Benaroya_et_al_2006.pdf:PDF},
  keywords = {separation, speaker},
  owner = {ozerov},
  timestamp = {2009.03.12}
}

@INPROCEEDINGS{Benaroya2006a,
  author = {L.~Benaroya and R.~Blouet and C.~{F{\'e}votte} and I.~Cohen},
  title = {Single sensor source separation using multiple-window {STFT} representation},
  booktitle = {Proc. International Workshop on Acoustic Echo and Noise Control (IWAENC'06)},
  year = {2006},
  address = {Paris, France},
  month = {Sep. 12--14},
  owner = {Aliosha},
  timestamp = {2010.11.04}
}

@INPROCEEDINGS{Benaroya2003,
  author = {L. Benaroya and R. Gribonval and F. Bimbot},
  title = {Non negative sparse representation for {W}iener based source separation
	with a single sensor},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal
	Processing (ICASSP'03)},
  year = {2003},
  pages = {613--616},
  address = {Hong Kong},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@INPROCEEDINGS{Benetos2006,
  author = {E. Benetos and C. Kotropoulos and T. Lidy and A. Rauber},
  title = {Testing supervised classifiers based on non-negative matrix factorization
	to musical instrument classification},
  booktitle = {Proc. 14th European Signal Processing Conf.},
  year = {2006},
  month = {September},
  file = {Benetos_et_al_2006.pdf:Benetos_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Benetos2005,
  author = {Emmanouil Benetos and Margarita Kotti and Constantine Kotropoulos
	and Juan José Burred and Gunnar Eisenberg and Martin Haller and Thomas
	Sikora},
  title = {Comparison of Subspace Analysis-based and Statistical Model-based
	Algorithms for Musical Instrument Classification},
  booktitle = {Proceedings of 2nd Workshop on Immersive Communication and Broadcast
	Systems (ICOB '05)},
  year = {2005},
  address = {Berlin, Germany},
  month = oct # { 27-28},
  abstract = {In this paper, three classes of algorithms for automatic classification
	of individual musical instrument sounds are compared. The first class
	of classifiers is based on Non-negative Matrix Factorization, the
	second class of classifiers employs automatic feature selection and
	Gaussian Mixture Models and the third is based on continuous Hidden
	Markov Models. Several perceptual features used in general sound
	classification as well as MPEG-7 basic spectral and spectral basis
	descriptors were measured for 300 sound recordings consisting of
	6 different musical instrument classes (piano, violin, cello, flute,
	bassoon, and soprano saxophone) from the University of Iowa database.
	The audio files were split using 70% of the available data for training
	and the remaining 30% for testing. Experimental results are presented
	to compare the classifier performance. The results indicate that
	all algorithm classes offer an accuracy of over 95% that outperforms
	the state-of-the-art performance reported for the aforementioned
	experiment.},
  file = {Benetos_et_al_2005.pdf:Benetos_et_al_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Bertin2007,
  author = {Bertin, N. and Badeau, R. and Richard, G.},
  title = {Blind Signal Decompositions for Automatic Transcription of Polyphonic
	Music: {NMF} and {K-SVD} on the Benchmark},
  booktitle = {Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE
	International Conference on},
  year = {2007},
  volume = {1},
  pages = {I--65--I--68},
  month = {15-20 April},
  abstract = {This paper investigates on the behavior of two blind signal decomposition
	algorithms, non negative matrix factorization (NMF) and non negative
	K-SVD (NKSVD), in a polyphonic music transcription task. State-of-the-art
	transcription systems are based on a frame-by-frame, low-level approach;
	blind systems could be an alternative to them. Two raw but effective
	audio-to-MIDI systems are proposed and evaluated. Performances are
	similar, but in favor of NMF, which is more robust to initialization,
	choice of the order and computationally less costly.},
  doi = {10.1109/ICASSP.2007.366617},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@ARTICLE{Bertin2010,
  author = {Bertin, N. and Badeau, R. and Vincent, E. },
  title = {Enforcing Harmonicity and Smoothness in Bayesian Non-Negative Matrix
	Factorization Applied to Polyphonic Music Transcription},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2010},
  volume = {18},
  pages = {538--549},
  number = {3},
  abstract = {This paper presents theoretical <span class='snippet'>and</span> experimental
	results about constrained <span class='snippet'>non</span>-<span
	class='snippet'>negative</span> <span class='snippet'>matrix</span>
	<span class='snippet'>factorization</span> (NMF) <span class='snippet'>in</span>
	a <span class='snippet'>Bayesian</span> framework. A model of superimposed
	Gaussian components including <span class='snippet'>harmonicity</span>
	is proposed, while temporal continuity is <span class='snippet'>enforced</span>
	through an inverse-Gamma Markov chain prior. We then exhibit a space-alternating
	generalized expectation-maximization (SAGE) algorithm to estimate
	the parameters. Computational time is reduced by initializing the
	system with an original variant of multiplicative harmonic NMF, which
	is described as well. The algorithm is then <span class='snippet'>applied</span>
	to perform polyphonic piano music transcription. It is compared to
	other state-of-the-art algorithms, especially NMF-based. Convergence
	issues are also discussed on a theoretical <span class='snippet'>and</span>
	experimental point of view. <span class='snippet'>Bayesian</span>
	NMF with <span class='snippet'>harmonicity</span> <span class='snippet'>and</span>
	temporal continuity constraints is shown to outperform other standard
	NMF-based transcription systems, providing a meaningful mid-level
	representation of the data. However, temporal <span class='snippet'>smoothness</span>
	has its drawbacks, as far as transients are concerned <span class='snippet'>in</span>
	particular, <span class='snippet'>and</span> can be detrimental to
	transcription performance when it is the only constraint used. Possible
	improvements of the temporal prior are discussed.},
  doi = {10.1109/TASL.2010.2041381},
  owner = {ozerov},
  timestamp = {2010.08.30}
}

@INPROCEEDINGS{Blandin2011,
  author = {C. Blandin and E. Vincent and A. Ozerov},
  title = {Multi-source {TDOA} estimation using {SNR}-based angular spectra},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'11)},
  year = {2011},
  pages = {2616 -- 2619},
  address = {Prague, Czech Republic},
  month = {May},
  owner = {Aliosha},
  timestamp = {2010.11.03}
}

@INPROCEEDINGS{Blouet2008,
  author = {R. Blouet and G. Rapaport and I. Cohen and C. F{\'e}votte},
  title = {Evaluation of several strategies for single sensor speech/music separation},
  booktitle = {Proc. International Conference on Acoustics, Speech and Signal Processing
	(ICASSP'08)},
  year = {2008},
  pages = {37 -- 40},
  address = {Las Vegas, USA},
  month = {Apr.},
  file = {Blouet_et_al_2008.pdf:Blouet_et_al_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.03.23}
}

@ARTICLE{Bos1994,
  author = {A. van den Bos},
  title = {Complex gradient and {H}essian},
  journal = {IEE Proceedings on Vision, Image and Signal Processing},
  year = {1994},
  volume = {141},
  pages = {380--382},
  file = {van_den_Bosget_1994.pdf:van_den_Bosget_1994.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.08}
}

@ARTICLE{Brown1999,
  author = {J. C. Brown},
  title = {Computer identification of musical instruments using pattern recognition
	with cepstral coefficients as features},
  journal = {The Journal of the Acoustical Society of America},
  year = {1999},
  volume = {105},
  pages = {1933},
  number = {3},
  file = {Brown_1997.pdf:Brown_1997.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Cardoso2001,
  author = {J.-F. Cardoso},
  title = {The three easy routes to independent component analysis; contrasts
	and geometry},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'01)},
  year = {2001},
  pages = {1--6},
  address = {San Diego, USA},
  month = {Dec.},
  owner = {Aliosha},
  timestamp = {2011.05.22}
}

@ARTICLE{Cardoso2008,
  author = {Cardoso, J.-F. and Le Jeune, M. and Delabrouille, J. and Betoule,
	M. and Patanchon, G.},
  title = {Component Separation With Flexible Models --- {A}pplication to Multichannel
	Astrophysical Observations},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year = {2008},
  volume = {2},
  pages = {735--746},
  number = {5},
  abstract = {This paper offers a new point of view on <span class='snippet'>component</span>
	<span class='snippet'>separation</span>, based on a <span class='snippet'>model</span>
	of additive <span class='snippet'>components</span> much more <span
	class='snippet'>flexible</span> than what is used in more traditional
	approaches. This flexibility is needed to accommodate the complex
	full-sky observations expected from the Planck space mission, for
	which it was developed, but it may also be useful in any context
	where accurate <span class='snippet'>component</span> <span class='snippet'>separation</span>
	is needed.},
  doi = {10.1109/JSTSP.2008.2005346},
  owner = {ozerov},
  timestamp = {2010.08.30}
}

@INPROCEEDINGS{Cardoso2007,
  author = {Jean-Francois Cardoso and Maude Martin},
  title = {A Flexible Component Model for Precision {ICA}},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'07)},
  year = {2007},
  pages = {1-8},
  file = {Cardoso_and_Martin_2007.pdf:Cardoso_and_Martin_2007.pdf:PDF}
}

@INPROCEEDINGS{Cardoso2004,
  author = {J.-F. Cardoso and D.-T. Pham},
  title = {Optimization Issues in Noisy {G}aussian {ICA}},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'04)},
  year = {2004},
  pages = {41--48},
  file = {Cardoso_and_Pham_2004.pdf:Cardoso_and_Pham_2004.pdf:PDF}
}

@INPROCEEDINGS{Cardoso2002,
  author = {J.-F.~Cardoso and H.~Snoussi and J.~Delabrouille and G.~Patanchon},
  title = {Blind separation of noisy {G}aussian stationary sources. {A}pplication
	to cosmic microwave background imaging},
  booktitle = {Proc.~11\textsuperscript{e} European Signal Processing Conference
	(EUSIPCO'02)},
  year = {2002},
  pages = {561--564},
  file = {Cardoso_et_al_2002.pdf:Cardoso_et_al_2002.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.08}
}

@ARTICLE{Carobbi2008,
  author = {C. F. M. Carobbi and M. Cati},
  title = {The Absolute Maximum of the Likelihood Function of the Rice Distribution:
	Existence and Uniqueness},
  journal = {IEEE Transactions on Instrumentation and Measurements},
  year = {2008},
  volume = {57},
  pages = {682--689},
  number = {4},
  month = {April},
  file = {Carobbi_and_Cati_2008.pdf:Carobbi_and_Cati_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Cemgil2003,
  author = {Cemgil, A.T. and Kappen, B. and Barber, D.},
  title = {Generative model based polyphonic music transcription},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2003 IEEE
	Workshop on.},
  year = {2003},
  pages = {181--184},
  month = {19-22 Oct.},
  abstract = {We present a model for simultaneous tempo and polyphonic pitch tracking.
	Our model, a form of dynamic Bayesian network (Murphy, K.P., 2002),
	embodies a transparent and computationally tractable approach to
	this acoustic analysis problem. An advantage of our approach is that
	it places emphasis on modeling the sound generation procedure. It
	provides a clear framework in which both high level (cognitive) prior
	information on music structure can be coupled with low level (acoustic
	physical) information in a principled manner to perform the analysis.
	The model is readily extensible to more complex sound generation
	processes.},
  owner = {ozerov},
  timestamp = {2008.11.10}
}

@ARTICLE{Cemgil2009,
  author = {Cemgil, A. T.},
  title = {Bayesian Inference in Non-negative Matrix Factorisation Models},
  journal = {Computational Intelligence and Neuroscience},
  year = {2009},
  number = {Article ID 785152},
  doi = {http://dx.doi.org/10.1155/2009/785152},
  owner = {Aliosha},
  timestamp = {2010.10.19}
}

@ARTICLE{Cemgil2006,
  author = {A. T. Cemgil and H. J. Kappen and D. Barber},
  title = {A {G}enerative {M}odel for {M}usic {T}ranscription},
  journal = {{IEEE} Transactions on Audio, Speech and Language Processing},
  year = {2006},
  volume = {14},
  pages = {679--694},
  number = {2},
  month = {March},
  abstract = {In this paper we present a graphical model for polyphonic music transcription.
	Our model, formulated as a Dynamical Bayesian Network, embodies a
	transparent and computationally tractable approach to this acoustic
	analysis problem. An advantage of our approach is that it places
	emphasis on explicitly modelling the sound generation procedure.
	It provides a clear framework in which both high level (cognitive)
	prior information on music structure can be coupled with low level
	(acoustic physical) information in a principled manner to perform
	the analysis. The model is a special case of the, generally intractable,
	Switching Kalman Filters. Where possible, we derive, exact polynomial
	time inference procedures, and otherwise efficient approximations.
	We argue that our generative model based approach is computationally
	feasible for many music applications and is readily extensible to
	more general auditory scene analysis scenarios.},
  file = {Cemgil_et_al_2006.pdf:Cemgil_et_al_2006.pdf:PDF},
  optkey = {10.1109/TSA.2005.852985},
  owner = {ozerov},
  timestamp = {2008.11.10}
}

@MANUAL{Chang2001,
  title = {{LIBSVM}: a library for support vector machines},
  author = {Chih-Chung Chang and Chih-Jen Lin},
  year = {2001},
  note = {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}},
  owner = {ozerov},
  timestamp = {2008.10.30}
}

@INPROCEEDINGS{Cichocki2006,
  author = {A.~Cichocki and R.~Zdunek and S. Amari},
  title = {{C}siszar's Divergences for Non-Negative Matrix Factorization: Family
	of New Algorithms},
  booktitle = {Proc.~6th International Conference on Independent Component Analysis
	and Blind Signal Separation (ICA'06)},
  year = {2006},
  pages = {32-39},
  address = {Charleston SC, USA},
  owner = {ozerov},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{Cont2007,
  author = {A. Cont and S. Dubnov and D. Wessel},
  title = {Realtime Multiple-pitch and Multiple-instrument Recognition for Music
	Signals using Sparse Non-negative Constraints},
  booktitle = {Digital Audio Effects (DAFx)},
  year = {2007},
  address = {Bordeaux},
  file = {Cont_et_al_2007.pdf:Cont_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@ARTICLE{Cooke2001,
  author = {M. P. Cooke and P. D. Green and L. Josifovski and A. Vizinho},
  title = {Robust Automatic Speech Recognition with Missing and Unreliable Acoustic
	Data},
  journal = {Speech Communication},
  year = {2001},
  volume = {34},
  pages = {267--285},
  file = {Cooke_et_al_2001.pdf:Cooke_et_al_2001.pdf:PDF},
  owner = {ozerov},
  timestamp = {2007.12.14}
}

@ARTICLE{Dempster1977,
  author = {A. P. Dempster and N. M. Laird and D. B. Rubin.},
  title = {Maximum likelihood from incomplete data via the {EM} algorithm},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  year = {1977},
  volume = {39},
  pages = {1--38},
  file = {Dempster_et_al_1977.pdf:Dempster_et_al_1977.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.16}
}

@ARTICLE{Deng2005,
  author = {Li Deng and Droppo, J. and Acero, A.},
  title = {Dynamic compensation of {HMM} variances using the feature enhancement
	uncertainty computed from a parametric model of speech distortion},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {2005},
  volume = {13},
  pages = {412- 421},
  abstract = {This paper presents a new technique for dynamic, frame-by-frame compensation
	of the Gaussian variances in the hidden Markov model (HMM), exploiting
	the feature variance or uncertainty estimated during the speech feature
	enhancement process, to improve noise-robust speech recognition.
	The new technique provides an alternative to the Bayesian predictive
	classification decision rule by carrying out an integration over
	the feature space instead of over the model-parameter space, offering
	a much simpler system implementation, lower computational cost, and
	dynamic compensation capabilities at the frame level. The computation
	of the feature enhancement variances is carried out using a probabilistic
	and parametric model of speech distortion, free from the use of any
	stereo training data. Dynamic compensation of the Gaussian variances
	in the HMM recognizer is derived, which is simply enlarging the HMM
	Gaussian variances by the feature enhancement variances. Experimental
	evaluation using the full Aurora2 test data sets demonstrates a significant
	digit error rate reduction, averaged over all noisy and signal-to-noise-ratio
	conditions, compared with the baseline that did not exploit the enhancement
	variance information. When the true enhancement variances are used,
	further dramatic error rate reduction is observed, indicating the
	strong potential for the new technique and the strong need for high
	accuracy in estimating the variances associated with feature enhancement.
	All the results, using either the true variances of the enhanced
	features or the estimated ones, show that the greatest contribution
	to recognizer's performance improvement is due to the use of the
	uncertainty for the static features, next due to the delta features,
	and the least due to the delta-delta features.},
  file = {Deng_et_al_2005.pdf:Deng_et_al_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2007.12.18}
}

@CONFERENCE{Duong2009,
  author = {NQ Duong and E Vincent and R Gribonval},
  title = {Spatial covariance models for under-determined reverberant audio
	source separation},
  booktitle = {Proc. IEEE Workshop on Applications of Signal Processing to Audio
	and Acoustics (WASPAA '09)},
  year = {2009},
  pages = {129--132},
  address = {New Paltz, NY},
  owner = {ozerov},
  timestamp = {2010.03.22}
}

@INPROCEEDINGS{Duong2010,
  author = {N. Q. K. Duong and E. Vincent and R. Gribonval},
  title = {Under-determined convolutive blind source separation using spatial
	covariance models},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal
	Processing (ICASSP)},
  year = {2010},
  pages = {9 -- 12},
  month = {Mar},
  owner = {ozerov},
  timestamp = {2010.04.15}
}

@INPROCEEDINGS{Duong2010a,
  author = {N Q K Duong and E Vincent and R Gribonval},
  title = {Under-determined reverberant audio source separation using local
	observed covariance and auditory-motivated time-frequency representation},
  booktitle = {9th International Conference on Latent Variable Analysis and Signal
	Separation (LVA/ICA'10)},
  year = {2010},
  pages = {73--80},
  address = {Saint-Malo, France},
  month = {Sep. 27-30},
  owner = {ozerov},
  timestamp = {2010.08.30}
}

@ARTICLE{Duong2011,
  author = {N. Q. K. Duong and E. Vincent and R. Gribonval},
  title = {Under-determined reverberant audio source separation using a full-rank
	spatial covariance model},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2010},
  volume = {18},
  pages = {1830--1840},
  number = {7},
  month = {Sep.},
  owner = {ozerov},
  timestamp = {2010.08.30}
}

@ARTICLE{Durrieu2010,
  author = {J.~L.~Durrieu and G.~Richard and B.~David and C.~F\'evotte},
  title = {Source/Filter Model for Unsupervised Main Melody Extraction From
	Polyphonic Audio Signals},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2010},
  volume = {18},
  pages = {564--575},
  number = {3},
  doi = {http://dx.doi.org/10.1109/TASL.2010.2041114},
  owner = {ozerov},
  timestamp = {2010.04.12}
}

@INPROCEEDINGS{Durrieu2009,
  author = {J.-L. Durrieu and G. Richard and B. David},
  title = {An Iterative Approach to Monaural Musical Mixture De-Soloing},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'09)},
  year = {2009},
  owner = {ozerov},
  timestamp = {2009.03.20}
}

@INPROCEEDINGS{Eggert2004,
  author = {Eggert, J. and K\"{o}rner, E.},
  title = {Sparse coding and {NMF}},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks
	(IJCNN'04)},
  year = {2004},
  pages = {2529--2533},
  owner = {Aliosha},
  timestamp = {2010.11.03}
}

@INPROCEEDINGS{Eggink2003a,
  author = {Eggink, J. and Brown, G.J.},
  title = {A missing feature approach to instrument identification in polyphonic
	music},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2003 IEEE
	Workshop on.},
  year = {2003},
  pages = {49},
  month = {19-22 Oct.},
  abstract = {Summary form only given. Gaussian mixture model (GMM) classifiers
	have been shown to give good instrument recognition performance for
	monophonic music played by a single instrument. However, many applications
	(such as automatic music transcription) require instrument identification
	from polyphonic, multi-instrumental recordings. We address this problem
	by incorporating ideas from missing feature theory into a GMM classifier.
	Specifically, frequency regions that are dominated by energy from
	an interfering tone are marked as unreliable and excluded from the
	classification process. This approach has been evaluated on random
	two-tone chords and an excerpt from a commercially available compact
	disc, with promising results.},
  file = {Eggink_and_Brown_2003a.pdf:Eggink_and_Brown_2003a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.12}
}

@INPROCEEDINGS{Eggink2003,
  author = {J. Eggink and G. J. Brown},
  title = {Application of missing feature theory to the recognition of musical
	instruments in polyphonic audio},
  booktitle = {Proceedings of International Symposium on Music Information Retrieval
	(ISMIR '03)},
  year = {2003},
  address = {Baltimore, Md, USA},
  month = {October},
  file = {Eggink_and_Brown_2003.PDF:Eggink_and_Brown_2003.PDF:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@TECHREPORT{Eguchi2001,
  author = {S. Eguchi and Y. Kano},
  title = {Robustifying maximum likelihood estimation},
  institution = {Institute of Statistical Mathematics},
  year = {2001},
  month = {June},
  note = {Research Memo. 802.},
  owner = {ozerov},
  timestamp = {2008.11.19}
}

@INPROCEEDINGS{ElChami2008,
  author = {Z. {El Chami} and D.~T Pham and C. Servi\`ere and A. Gu\'erin},
  title = {A new model-based underdetermined speech separation},
  booktitle = {Proc. 11th International Workshop on Acoustic Echo and Noise Control
	(IWAENC'08)},
  year = {2008},
  pages = {1--4},
  abstract = {Full-rank uniform prior of source spatial covariance matrix has been
	considered. DUET ratio in each T-F point is considered as a random
	variable with a certain distribution to take in to account the reverberation.
	Model parameters are estimated by EM algorithm},
  owner = {ozerov},
  timestamp = {2010.04.02}
}

@ARTICLE{Emiya2010,
  author = {Emiya, V. and Vincent, E. and Harlander, N. and Hohmann, V.},
  title = {Subjective and objective quality assessment of audio source separation},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  note = {submitted},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Eronen2001,
  author = {Eronen, A.},
  title = {Comparison of features for musical instrument recognition},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2001 IEEE
	Workshop on the},
  year = {2001},
  pages = {19--22},
  month = {21-24 Oct.},
  abstract = {Several features were compared with regard to recognition performance
	in a musical instrument recognition system. Both mel-frequency and
	linear prediction cepstral and delta cepstral coefficients were calculated.
	Linear prediction analysis was carried out both on a uniform and
	a warped frequency scale, and reflection coefficients were also used
	as features. The performance of earlier described features relating
	to the temporal development, modulation properties, brightness, and
	spectral synchronity of sounds was also analysed. The data base consisted
	of 5286 acoustic and synthetic solo tones from 29 different Western
	orchestral instruments, out of which 16 instruments were included
	in the test set. The best performance for solo tone recognition,
	35% for individual instruments and 77% for families, was obtained
	with a feature set consisting of two sets of mel-frequency cepstral
	coefficients and a subset of the other analysed features. The confusions
	made by the system were analysed and compared to results reported
	in a human perception experiment},
  doi = {10.1109/ASPAA.2001.969532},
  file = {Eronen_2001.pdf:Eronen_2001.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Eronen2000,
  author = {Eronen, A. and Klapuri, A.},
  title = {Musical instrument recognition using cepstral coefficients and temporal
	features},
  booktitle = {Acoustics, Speech, and Signal Processing, 2000. ICASSP '00. Proceedings.
	2000 IEEE International Conference on},
  year = {2000},
  volume = {2},
  pages = {753--756},
  month = {June},
  abstract = {In this paper, a system for pitch independent musical instrument recognition
	is presented. A wide set of features covering both spectral and temporal
	properties of sounds was investigated, and their extraction algorithms
	were designed. The usefulness of the features was validated using
	test data that consisted of 1498 samples covering the full pitch
	ranges of 30 orchestral instruments from the string, brass and woodwind
	families, played with different techniques. The correct instrument
	family was recognized with 94% accuracy and individual instruments
	in 80% of cases. These results are compared to those reported in
	other work. Also, utilization of a hierarchical classification framework
	is considered},
  doi = {10.1109/ICASSP.2000.859069},
  file = {Eronen_and_Klapuri_2000.pdf:Eronen_and_Klapuri_2000.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@PHDTHESIS{Essid2005a,
  author = {Slim Essid},
  title = {Classification automatique des signaux audio-fr\'equences : reconnaissance
	des instruments de musique},
  school = {Universit\'e Pierre et Marie Curie},
  year = {2005},
  file = {Essid_PhD.pdf:Essid_PhD.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.04}
}

@ARTICLE{Essid2006,
  author = {S. Essid and G. Richard and B. David},
  title = {Instrument Recognition in Polyphonic Music based on Automatic Taxonomies},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2006},
  volume = {14},
  pages = {68--80},
  number = {1},
  file = {Essid_et_al_2006.pdf:Essid_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.03.20}
}

@INPROCEEDINGS{Essid2005,
  author = {Essid, S. and Richard, G. and David, B.},
  title = {Instrument recognition in polyphonic music},
  booktitle = {Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP
	'05). IEEE International Conference on},
  year = {2005},
  volume = {3},
  pages = {245--248},
  month = {March},
  abstract = {We propose a method for the recognition of musical instruments in
	polyphonic music excerpted from commercial recordings. By exploiting
	some cues on the common structures of musical ensembles, we show
	that it is possible to recognize up to 4 instruments playing concurrently.
	The system associates a hierarchical classification tree with a class-pairwise
	feature selection technique and Gaussian mixture models to discriminate
	possible combinations of instruments. Successful identification is
	achieved over short-time windows, enabling the system to be employed
	for segmentation purposes.},
  doi = {10.1109/ICASSP.2005.1415692},
  file = {Essid_et_al_2005.pdf:Essid_et_al_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Essid2004,
  author = {S. Essid and G. Richard and B. David},
  title = {Musical Instrument Recognition on solo performance},
  booktitle = {European Signal Processing Conference (EUSIPCO)},
  year = {2004},
  address = {Vienna, Austria},
  month = {September},
  file = {Essid_et_al_2004.pdf:Essid_et_al_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Essid2004a,
  author = {S. Essid and G. Richard and B. David},
  title = {Musical instrument recognition based on class pairwise feature selection},
  booktitle = {5th International Conference on Music Information Retrieval (ISMIR)},
  year = {2004},
  address = {Barcelona, Spain},
  month = {October},
  file = {Essid_et_al_2004a.pdf:Essid_et_al_2004a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Fevotte2005,
  author = {C.~F\'evotte and J.-F.~Cardoso},
  title = {Maximum likelihood approach for blind audio source separation using
	time-frequency {G}aussian source models},
  booktitle = {Proc. IEEE Workshop on Applications of Signal Processing to Audio
	and Acoustics (WASPAA '05)},
  year = {2005},
  pages = {78--81},
  address = {Mohonk, NY, USA},
  month = {Oct.},
  owner = {ozerov},
  timestamp = {2008.09.23}
}

@ARTICLE{FitzGerald2008,
  author = {D. FitzGerald and M. Cranitch and E. Coyle},
  title = {Extended Nonnegative Tensor Factorisation Models for Musical Sound
	Source Separation},
  journal = {Computational Intelligence and Neuroscience. Hindawi Publishing Corp},
  year = {2008},
  volume = {2008},
  doi = {10.1155/2008/872425},
  owner = {ozerov},
  timestamp = {2010.03.19}
}

@INPROCEEDINGS{FitzGerald2006,
  author = {FitzGerald, D. and Cranitch, M. and Coyle, E.},
  title = {Shifted 2D Non-negative Tensor Factorisation},
  booktitle = {Proceedings of the Irish Signals and Systems Conference},
  year = {2006},
  address = {Dublin},
  month = {June},
  file = {FitzGerald_et_al_2006.pdf:FitzGerald_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.13}
}

@INPROCEEDINGS{FitzGerald2006a,
  author = {FitzGerald, D. and Cranitch, M. and Coyle, E.},
  title = {Sound Source Separation Using Shifted Non-Negative Tensor Factorisation},
  booktitle = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings.
	2006 IEEE International Conference on},
  year = {2006},
  volume = {5},
  pages = {V--V},
  month = {May},
  doi = {10.1109/ICASSP.2006.1661360},
  file = {FitzGerald_et_al_2006a.pdf:FitzGerald_et_al_2006a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.13}
}

@INPROCEEDINGS{FitzGerald2005,
  author = {FitzGerald, D. and Cranitch, M. and Coyle, E.},
  title = {Shifted non-negative matrix factorisation for sound source separation},
  booktitle = {Statistical Signal Processing, 2005 IEEE/SP 13th Workshop on},
  year = {2005},
  pages = {1132--1137},
  month = {July},
  abstract = {A shifted non-negative matrix factorisation algorithm is derived,
	which offers advantages over previous matrix factorisation techniques
	for the purposes of single channel source separation. It represents
	a sound source as translations of a single frequency basis function.
	These translations approximately correspond to notes played by an
	instrument. Results are presented for a set of synthetic data, and
	on a single channel recording of piano and clarinet. Though the system
	is aimed at musical recordings, the technique can be applied to any
	data which contains shifted versions of an underlying factor, and
	so the algorithm could possibly be used in other applications such
	as image processing},
  doi = {10.1109/SSP.2005.1628765},
  file = {FitzGerald_et_al_2005.pdf:FitzGerald_et_al_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.13}
}

@INPROCEEDINGS{FitzGerald2005a,
  author = {FitzGerald, D. and Cranitch, M. and Coyle, E.},
  title = {Non-negative Tensor Factorisation for Sound Source Separation},
  booktitle = {Proc. of the Irish Signals and Systems Conference},
  year = {2005},
  address = {Dublin},
  month = {Sep.},
  file = {FitzGerald_et_al_2005a.pdf:FitzGerald_et_al_2005a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.13}
}

@INPROCEEDINGS{Fevotte2010,
  author = {C. F{\'e}votte and A. Ozerov},
  title = {Notes on nonnegative tensor factorization of the spectrogram for
	audio source separation : statistical insights and towards self-clustering
	of the spatial cues},
  booktitle = {7th International Symposium on Computer Music Modeling and Retrieval
	(CMMR 2010)},
  year = {2010},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@ARTICLE{Fevotte2008,
  author = {C.~{F{\'e}votte} and N.~Bertin and J.-L.~Durrieu},
  title = {Nonnegative matrix factorization with the {I}takura-{S}aito divergence.
	{W}ith application to music analysis},
  journal = {Neural Computation},
  year = {2009},
  volume = {21},
  pages = {793--830},
  number = {3},
  month = {Mar.},
  owner = {ozerov},
  timestamp = {2009.01.13}
}

@INPROCEEDINGS{Ganseman2010,
  author = {J. Ganseman and G. J. Mysore and J.S. Abel and P. Scheunders},
  title = {Source Separation by Score Synthesis},
  booktitle = {Proceedings of the International Computer Music Conference (ICMC)},
  year = {2010},
  address = {New York, NY},
  month = {June},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Gelly2005,
  author = {S. Gelly and N. Bredeche and M. Sebag},
  title = {From Factorial and Hierarchical {HMM} to {B}ayesian Network: A Representation
	Change Algorithm},
  booktitle = {SARA},
  year = {2005},
  pages = {107-120},
  owner = {ozerov},
  timestamp = {2010.12.21}
}

@ARTICLE{Ghahramani1997,
  author = {Z. Ghahramani and M. I. Jordan},
  title = {Factorial hidden {M}arkov models},
  journal = {Machine Learning},
  year = {1997},
  volume = {29},
  pages = {245--273},
  file = {Ghahramani_and_Jordan_1997.pdf:Ghahramani_and_Jordan_1997.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.03.12}
}

@ARTICLE{Gillet2008,
  author = {Gillet, O. and Richard, G.},
  title = {Transcription and Separation of Drum Signals From Polyphonic Music},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2008},
  volume = {16},
  pages = {529--540},
  number = {3},
  abstract = {The purpose <span class='snippet'>of</span> this article is to present
	new advances in <span class='snippet'>music</span> <span class='snippet'>transcription</span>
	<span class='snippet'>and</span> source <span class='snippet'>separation</span>
	with a focus on <span class='snippet'>drum</span> <span class='snippet'>signals</span>.
	A complete <span class='snippet'>drum</span> <span class='snippet'>transcription</span>
	system is described, which combines information <span class='snippet'>from</span>
	the original <span class='snippet'>music</span> <span class='snippet'>signal</span>
	<span class='snippet'>and</span> a <span class='snippet'>drum</span>
	track enhanced version obtained by source <span class='snippet'>separation</span>.
	In addition to efficient fusion strategies to take into account these
	two complementary sources <span class='snippet'>of</span> information,
	the <span class='snippet'>transcription</span> system integrates
	a large set <span class='snippet'>of</span> features, optimally selected
	by feature selection. Concurrently, the problem <span class='snippet'>of</span>
	<span class='snippet'>drum</span> track extraction <span class='snippet'>from</span>
	<span class='snippet'>polyphonic</span> <span class='snippet'>music</span>
	is tackled both by proposing a novel approach based on harmonic/noise
	decomposition <span class='snippet'>and</span> time/frequency masking
	<span class='snippet'>and</span> by improving an existing Wiener
	filtering-based <span class='snippet'>separation</span> method. The
	<span class='snippet'>separation</span> <span class='snippet'>and</span>
	<span class='snippet'>transcription</span> techniques presented are
	thoroughly evaluated on a large public database <span class='snippet'>of</span>
	<span class='snippet'>music</span> <span class='snippet'>signals</span>.
	A <span class='snippet'>transcription</span> accuracy between 64.5%
	<span class='snippet'>and</span> 80.3% is obtained, depending on
	the <span class='snippet'>drum</span> instrument, for well-balanced
	mixes, <span class='snippet'>and</span> the efficiency <span class='snippet'>of</span>
	our <span class='snippet'>drum</span> <span class='snippet'>separation</span>
	algorithms is illustrated in a comprehensive benchmark.},
  doi = {10.1109/TASL.2007.914120},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Goldberger2003,
  author = {Goldberger, J. and Gordon, S. and Greenspan, H.},
  title = {An efficient image similarity measure based on approximations of
	
	{KL}-divergence between two gaussian mixtures},
  booktitle = ICCV03,
  year = {2003},
  pages = {487--493},
  owner = {ozerov},
  timestamp = {2009.01.09}
}

@INPROCEEDINGS{Goto2004,
  author = {M. Goto and H. Hashiguchi and T. Nishimura and R. Oka},
  title = {{RWC} Music Database: {M}usic Genre Database and Musical Instrument
	Sound Databases},
  booktitle = {5th International Symposium on Music Information Retrieval (ISMIR)},
  year = {2004},
  pages = {229--230},
  owner = {ozerov},
  timestamp = {2011.06.09},
  url = {http://staff.aist.go.jp/m.goto/RWC-MDB/}
}

@INPROCEEDINGS{Grindlay2009,
  author = {Grindlay, G. and Ellis, D.},
  title = {Multi-voice polyphonic music transcription using eigeninstruments},
  booktitle = {Proc. IEEE Workshop Applications of Signal Processing to Audio and
	Acoustics (WASPAA '09)},
  year = {2009},
  pages = {53--56},
  abstract = {We present a model-based approach to separating and transcribing single-channel,
	<span class='snippet'>multi</span>-instrument <span class='snippet'>polyphonic</span>
	<span class='snippet'>music</span> in a semi-blind fashion. Our system
	extends the non-negative matrix factorization (NMF) algorithm to
	incorporate constraints on the basis vectors of the solution. In
	the context of <span class='snippet'>music</span> <span class='snippet'>transcription</span>,
	this allows us to encode prior knowledge about the space of possible
	instrument models as a parametric subspace we term ldquoeigeninstrumentsrdquo.
	We evaluate our algorithm on several synthetic (MIDI) recordings
	containing different instrument mixtures. Averaged over both sources,
	we achieved a frame-level accuracy of over 68% on an excerpt of Pachelbel's
	Canon arranged for doublebass and piano and 72% on a mixture of overlapping
	melodies played by flute and violin.},
  doi = {10.1109/ASPAA.2009.5346514},
  owner = {Aliosha},
  timestamp = {2010.10.18}
}

@INPROCEEDINGS{Hacihabibouglu2002,
  author = {H. Hac\i{}habibo\u{g}lu and N. Canagarajah},
  title = {Musical instrument recognition with wavelet envelopes},
  booktitle = {Proc. EAA Convention, Forum Acusticum Sevilla},
  year = {2002},
  address = {Sevilla, Spain},
  month = {16-20 September},
  file = {Hacihabiboglu_and_Canagarajah_2002.pdf:Hacihabiboglu_and_Canagarajah_2002.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Hastie1998,
  author = {Trevor Hastie and Robert Tibshirani},
  title = {Classification by pairwise coupling},
  booktitle = {The Annals of Statistics},
  year = {1998},
  pages = {507--513},
  publisher = {MIT Press},
  owner = {ozerov},
  timestamp = {2008.10.30}
}

@INPROCEEDINGS{Heittola2009,
  author = {Heittola, T. and Klapuri, A. and Virtanen, T.},
  title = {Musical Instrument Recognition in Polyphonic Audio Using Source-Filter
	Model for Sound Separation},
  booktitle = {Proc. 10th Int. Society for Music Information Retrieval Conf. (ISMIR
	2009)},
  year = {2009},
  address = {Kobe, Japan},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Helen2005,
  author = {M. Helén and T. Virtanen},
  title = {Separation of drums from polyphonic music using nonnegtive matrix
	factorization and support vector machine},
  booktitle = {Eur. Signal Process. Conf.},
  year = {2005},
  address = {Istanbul, Turkey},
  file = {Helen_and_Virtanen_2005.pdf:Helen_and_Virtanen_2005.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@ARTICLE{Hennequin2011,
  author = {Hennequin, R. and Badeau, R. and David, B.},
  title = {{NMF} With Time--Frequency Activations to Model Nonstationary Audio
	Events},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2011},
  volume = {19},
  pages = {744--753},
  number = {4},
  __markedentry = {[ozerov]},
  abstract = {Real-world sounds often exhibit time-varying spectral shapes, as observed
	in the spectrogram of a harpsichord tone or that of a transition
	between two pronounced vowels. Whereas the standard non-negative
	matrix factorization (NMF) assumes fixed spectral atoms, an extension
	is proposed where the temporal activations (coefficients of the decomposition
	on the spectral atom basis) become frequency dependent and follow
	a time-varying autoregressive moving average (ARMA) <span class='snippet'>modeling</span>.
	This extension can thus be interpreted with the help of a source/filter
	paradigm and is referred to as source/filter factorization. This
	factorization leads to an efficient single-atom decomposition for
	a single <span class='snippet'>audio</span> <span class='snippet'>event</span>
	with strong spectral variation (but with constant pitch). The new
	algorithm is tested on real <span class='snippet'>audio</span> data
	and shows promising results.},
  doi = {10.1109/TASL.2010.2062506},
  owner = {ozerov},
  timestamp = {2011.06.07}
}

@ARTICLE{Hlawatsch1992,
  author = {Hlawatsch, F. and Boudreaux-Bartels, G. F.},
  title = {Linear and quadratic time-frequency signal representations},
  journal = {IEEE Signal Processing Magazine},
  year = {1992},
  volume = {9},
  pages = {21--67},
  number = {2},
  doi = {10.1109/79.127284},
  owner = {Aliosha},
  timestamp = {2010.11.06}
}

@ARTICLE{Hoyer2004,
  author = {P. O. Hoyer},
  title = {Non-negative matrix factorization with sparseness constraints},
  journal = {Journal of Machine Learning Research},
  year = {2004},
  volume = {5},
  pages = {1457--1469},
  owner = {Aliosha},
  timestamp = {2010.11.03}
}

@INPROCEEDINGS{Izumi2007,
  author = {Izumi, Yosuke and Ono, Nobutaka and Sagayama, Shigeki},
  title = {Sparseness-Based {2CH BSS} using the {EM} Algorithm in Reverberant
	Environment},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2007 IEEE
	Workshop on},
  year = {2007},
  pages = {147--150},
  month = {21-24 Oct.},
  abstract = {In this paper, we propose a new approach to sparseness-based BSS based
	on the EM algorithm, which iteratively estimates the DOA and the
	time-frequency mask for each source through the EM algorithm under
	the sparseness assumption. Our method has the following characteristics:
	1) it enables the introduction of physical observation models such
	as the diffuse sound field, because the likelihood is defined in
	the original signal domain and not in the feature domain, 2) one
	does not necessarily have to know in advance the power of the background
	noise since they are also parameters which can be estimated from
	the observed signal, 3) it takes short computational time, 4) a common
	objective function is iteratively increased in localization and separation
	steps, which correspond to the E-step and M-step, respectively. Although
	our framework is applicable to general N channel BSS, we will concentrate
	on the formulation of the problem in the particular case where two
	sensory inputs are available, and we show some numerical simulation
	results.},
  doi = {10.1109/ASPAA.2007.4393015},
  file = {Izumi_et_al_2007.pdf:Izumi_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.04}
}

@ARTICLE{Joder2008,
  author = {C. Joder and S. Essid and G. Richard},
  title = {Temporal Integration for Audio Classification with Application to
	Musical Instrument Classification},
  journal = {Submitted to IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2008},
  owner = {ozerov},
  timestamp = {2008.07.07}
}

@ARTICLE{Jordan1999,
  author = {M. I. Jordan and Z. Ghahramani and T. S. Jaakkola and L. K. Saul},
  title = {An introduction to variational methods for graphical models},
  journal = {Learning in Graphical Models},
  year = {1999},
  volume = {37},
  pages = {183--233},
  number = {2},
  owner = {Loha},
  timestamp = {2008.01.02}
}

@ARTICLE{Juang1990,
  author = {B.-H. Juang and L. Rabiner},
  title = {The segmental {K}-means algorithm for estimating parameters of hidden
	{M}arkov models},
  journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  year = {1990},
  volume = {38},
  pages = {1639--1641},
  file = {Juang_and_Rabiner_1990.pdf:Juang_and_Rabiner_1990.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.06.10}
}

@ARTICLE{Kameoka2007,
  author = {Hirokazu Kameoka and Takuya Nishimoto and Shigeki Sagayama},
  title = {A Multipitch Analyzer Based on Harmonic Temporal Structured Clustering},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2007},
  volume = {15},
  pages = {982--994},
  number = {3},
  doi = {10.1109/TASL.2006.885248},
  owner = {ozerov},
  timestamp = {2010.09.10}
}

@ARTICLE{Kashino1999,
  author = {K. Kashino and H. Murase},
  title = {A sound source identification system for ensemble music based on
	template adaptation and music stream extraction},
  journal = {Speech Communication},
  year = {1999},
  volume = {27},
  pages = {337--349},
  month = {April},
  doi = {doi:10.1016/S0167-6393(98)00078-8},
  owner = {ozerov},
  timestamp = {2008.02.12},
  url = {http://www.ingentaconnect.com/content/els/01676393/1999/00000027/00000003/art00078}
}

@BOOK{Kay1993,
  title = {Fundamentals of Statistical Signal Processing: Estimation Theory},
  publisher = {Prentice Hall},
  year = {1993},
  author = {Kay, S. M.},
  address = {Englewood Cliffs, NJ},
  owner = {ozerov},
  timestamp = {2008.12.13}
}

@INPROCEEDINGS{Kim2008,
  author = {Y. E. Kim and J. M. Walsh and T. M. Doll},
  title = {Comparison of a Joint Iterative Method for Multiple Speaker Identification
	with Sequential Blind Source Separation and Speaker Identification},
  booktitle = {Odyssey 2008 : The Speaker and Language Recognition Workshop},
  year = {2008},
  month = {Jan.},
  file = {Kim_et_al_2008.pdf:Kim_et_al_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@PHDTHESIS{Kitahara2007,
  author = {Tetsuro Kitahara},
  title = {Computational Musical Instrument Recognition and Its Application
	to Content-based Music Information Retrieval},
  school = {Kyoto University},
  year = {2007},
  address = {Japan},
  month = {March},
  file = {Kitahara_PhD_Thesis.pdf:Kitahara_PhD_Thesis.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11},
  url = {http://winnie.kuis.kyoto-u.ac.jp/~kitahara/papers/d-thesis-kitahara.pdf}
}

@INPROCEEDINGS{Kitahara2006,
  author = {Kitahara, T. and Goto, M. and Komatani, K. and Ogata, T. and Okuno,
	H.G.},
  title = {Instrogram: A New Musical Instrument Recognition Technique Without
	Using Onset Detection NOR F0 Estimation},
  booktitle = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings.
	2006 IEEE International Conference on},
  year = {2006},
  volume = {5},
  pages = {V--V},
  month = {14-19 May},
  abstract = {This paper describes a new technique for recognizing musical instruments
	in polyphonic music. Because the conventional framework for musical
	font color=990000>instrument/b> font color=990000>recognition/b>
	font color=990000>in/b> font color=990000>polyphonic/b> font color=990000>music/b>
	had to estimate the onset time and fundamental frequency (F0) of
	each note, instrument recognition strictly suffered from errors of
	onset detection and F0 estimation. Unlike such a note-based processing
	framework, our technique calculates the temporal trajectory of instrument
	existence probabilities for every possible F0, and the results are
	visualized with a spectrogram-like graphical representation called
	instrogram. The instrument existence probability is defined as the
	product of a nonspecific instrument existence probability calculated
	using PreFEst and a conditional instrument existence probability
	calculated using the hidden Markov model. Experimental results show
	that the obtained instrograms reflect the actual instrumentations
	and facilitate instrument recognition},
  doi = {10.1109/ICASSP.2006.1661254},
  file = {Kitahara_et_al_2006.pdf:Kitahara_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@ARTICLE{Kitahara2007a,
  author = {Tetsuro Kitahara and Masataka Goto and Kazunori Komatani and Tetsuya
	Ogata and Hiroshi G. Okuno},
  title = {Instrument Identification in Polyphonic Music: Feature Weighting
	to Minimize Influence of Sound Overlaps},
  journal = {EURASIP Journal on Applied Signal Processing},
  year = {2007},
  file = {Kitahara_et_al_2007.pdf:Kitahara_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Klapuri2007,
  author = {Klapuri, A.},
  title = {Analysis of Musical Instrument Sounds by Source-Filter-Decay Model},
  booktitle = {Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP'07)},
  year = {2007},
  volume = {1},
  pages = {53 -- 56},
  abstract = {This paper proposes a way <span class='snippet'>of</span> modelling
	the time-varying spectral energy distribution <span class='snippet'>of</span>
	<span class='snippet'>musical</span> <span class='snippet'>instrument</span>
	<span class='snippet'>sounds</span>. The model consists <span class='snippet'>of</span>
	an excitation signal, a body response <span class='snippet'>filter</span>,
	and a loss <span class='snippet'>filter</span> which implements a
	frequency-dependent <span class='snippet'>decay</span>. The three
	parts are further represented with a linear model which allows controlling
	the number <span class='snippet'>of</span> parameters involved. A
	method is proposed for estimating all the model parameters jointly,
	taking into account additive noise. The method is evaluated <span
	class='snippet'>by</span> measuring its accuracy in representing
	33 <span class='snippet'>musical</span> <span class='snippet'>instruments</span>
	and <span class='snippet'>by</span> testing its usefulness in extracting
	the melodic line <span class='snippet'>of</span> one <span class='snippet'>instrument</span>
	from a polyphonic audio signal.},
  doi = {10.1109/ICASSP.2007.366614},
  owner = {ozerov},
  timestamp = {2010.09.13}
}

@ARTICLE{Laurberg2008,
  author = {H. Laurberg and M. G. Christensen and M. D. Plumbley and L. K. Hansen
	and S. H. Jensen},
  title = {Theorems on Positive Data: {O}n the Uniqueness of {NMF}},
  journal = {Computational Intelligence and Neuroscience},
  year = {2008},
  volume = {2008},
  pages = {1--9},
  file = {Laurberg_et_al_2008.pdf:Laurberg_et_al_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.06.24}
}

@INPROCEEDINGS{Laurberg2008a,
  author = {H. Laurberg and M. N. Schmidt and M. G. Christensen and S. H. Jensen},
  title = {Structured non-negative matrix factorization with sparsity patterns},
  booktitle = {Proceedings Asilomar Conference on Signals, Systems, and Computers},
  year = {2008},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Lee2001,
  author = {D. D. Lee and H. S. Seung},
  title = {Algorithms for non-negative matrix factorization},
  booktitle = {Advances in Neural and Information Processing Systems 13},
  year = {2001},
  pages = {556--562},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@ARTICLE{Lee1999,
  author = {D. D. Lee and H. S. Seung},
  title = {Learning the parts of objects with nonnegative matrix factorization},
  journal = {Nature},
  year = {1999},
  volume = {401},
  pages = {788--791},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@INCOLLECTION{Lee2007,
  author = {Lee, I. and Kim, T. and Lee, T.-W.},
  title = {Independent vector analysis for convolutive blind speech separation},
  booktitle = {Blind speech separation},
  publisher = {Springer},
  year = {2007},
  pages = {169--192},
  owner = {Aliosha},
  timestamp = {2010.04.05}
}

@BOOK{Lee1998,
  title = {Independent Component Analysis - Theory and Applications},
  publisher = {Kluwer Academic Publishers},
  year = {1998},
  author = {T.-W. Lee},
  owner = {ozerov},
  timestamp = {2008.11.07}
}

@INPROCEEDINGS{Leveau2007,
  author = {Leveau, P. and Sodoyer, D. and Daudet, L.},
  title = {Automatic Instrument Recognition in a polyphonic mixture using Sparse
	Representations},
  booktitle = {8th International Conference on Music Information Retrieval (ISMIR
	2007)},
  year = {2007},
  address = {Vienna, Austria},
  month = {September 23-27},
  file = {Leveau_et_al_2007.pdf:Leveau_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Liutkus2010,
  author = {A. Liutkus and R. Badeau and G. Richard},
  title = {Informed source separation using latent components},
  booktitle = {9th International Conference on Latent Variable Analysis and Signal
	Separation (LVA/ICA'10)},
  year = {2010},
  address = {St Malo, France},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Livshin2004,
  author = {A. A. Livshin and X. Rodet},
  title = {Instrument recognition beyond separate notes - indexing continuous
	recordings},
  booktitle = {Proc. Int. Computer Music Conf.},
  year = {2004},
  file = {Livshin_and_Rodet_2004.pdf:Livshin_and_Rodet_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.12}
}

@INPROCEEDINGS{Livshin2004a,
  author = {A. A. Livshin and X. Rodet},
  title = {Musical instrument identification in continuous recordings},
  booktitle = {Proceedings of the 7th International Conference on Digital Audio
	Effects},
  year = {2004},
  pages = {222--226},
  file = {Livshin_and_Rodet_2004a.pdf:Livshin_and_Rodet_2004a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.12}
}

@BOOK{Makino2007,
  title = {Blind speech separation},
  publisher = {Springer},
  year = {2007},
  author = {S.~Makino and T.-W.~Lee and H.~Sawada},
  owner = {ozerov},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{Mandel2007,
  author = {M.I. Mandel and D.P.W. Ellis and T. Jebara},
  title = {An {EM} algorithm for localizing multiple sound sources in reverberant
	environments},
  booktitle = {Advances in Neural Information Processing Systems (NIPS 19)},
  year = {2007},
  file = {Mandel_et_al_2007.pdf:Mandel_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.04}
}

@ARTICLE{Mandel2010,
  author = {Mandel, M. I. and Weiss, R. J. and Ellis, D.},
  title = {Model-Based Expectation-Maximization Source Separation and Localization},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2010},
  volume = {18},
  pages = {382--394},
  number = {2},
  abstract = {This paper describes a system, referred to as <span class='snippet'>model</span>-<span
	class='snippet'>based</span> <span class='snippet'>expectation</span>-<span
	class='snippet'>maximization</span> <span class='snippet'>source</span>
	<span class='snippet'>separation</span> <span class='snippet'>and</span>
	<span class='snippet'>localization</span> (MESSL), for separating
	<span class='snippet'>and</span> localizing multiple sound <span
	class='snippet'>sources</span> from an underdetermined reverberant
	two-channel recording. By clustering individual spectrogram points
	<span class='snippet'>based</span> on their interaural phase <span
	class='snippet'>and</span> level differences, MESSL generates masks
	that can be used to isolate individual sound <span class='snippet'>sources</span>.
	We first describe a probabilistic <span class='snippet'>model</span>
	of interaural parameters that can be evaluated at individual spectrogram
	points. By creating a mixture of these models over <span class='snippet'>sources</span>
	<span class='snippet'>and</span> delays, the multi-<span class='snippet'>source</span>
	<span class='snippet'>localization</span> problem is reduced to a
	collection of single <span class='snippet'>source</span> problems.
	We derive an <span class='snippet'>expectation</span>-<span class='snippet'>maximization</span>
	algorithm for computing the maximum-likelihood parameters of this
	mixture <span class='snippet'>model</span>, <span class='snippet'>and</span>
	show that these parameters correspond well with interaural parameters
	measured in isolation. As a byproduct of fitting this mixture <span
	class='snippet'>model</span>, the algorithm creates probabilistic
	spectrogram masks that can be used for <span class='snippet'>source</span>
	<span class='snippet'>separation</span>. In simulated anechoic <span
	class='snippet'>and</span> reverberant environments, <span class='snippet'>separations</span>
	using MESSL produced on average a signal-to-distortion ratio 1.6
	dB greater <span class='snippet'>and</span> perceptual evaluation
	of speech quality (PESQ) results 0.27 mean opinion score units greater
	than four comparable algorithms.},
  doi = {10.1109/TASL.2009.2029711},
  owner = {Aliosha},
  timestamp = {2010.10.18}
}

@INPROCEEDINGS{Martin1997,
  author = {A. Martin and G. Doddington and T. Kamm and M. Ordowski and M. Przybocki},
  title = {The DET curve in assessment of detection task performance},
  booktitle = {European Conf. on Speech Communication and Technology (EuroSpeech'97)},
  year = {1997},
  pages = {1895--1898},
  owner = {ozerov},
  timestamp = {2008.10.30}
}

@MISC{Martin1998,
  author = {K. Martin},
  title = {Toward automatic sound source recognition: identifying musical instruments},
  year = {1998},
  file = {Martin_1998.pdf:Martin_1998.pdf:PDF},
  owner = {ozerov},
  text = {Martin, K. D. (1998). Toward automatic sound source recognition: identifying
	musical instruments. In Proc.},
  timestamp = {2008.02.11},
  url = {citeseer.ist.psu.edu/martin98toward.html}
}

@INPROCEEDINGS{Martin1998a,
  author = {K. Martin and Y. Kim},
  title = {Musical instrument identification: a pattern-recognition approach},
  booktitle = {Proc. 136th Meeting of the Acoustical Society of America},
  year = {1998},
  file = {Martin_and_Kim_1998.pdf:Martin_and_Kim_1998.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Martins2007,
  author = {Luis Gustavo Martins and Juan José Burred and George Tzanetakis and
	Mathieu Lagrange},
  title = {Polyphonic Instrument Recognition using Spectral Clustering},
  booktitle = {8th International Conference on Music Information Retrieval (ISMIR2007)},
  year = {2007},
  address = {Vienna, Austria},
  month = {September},
  file = {Martins_et_al_2007.pdf:Martins_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Marzetta1995,
  author = {Marzetta, T.L.},
  title = {{EM} algorithm for estimating the parameters of a multivariate complex
	{R}ician density for polarimetric {SAR}},
  booktitle = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995
	International Conference on},
  year = {1995},
  volume = {5},
  pages = {3651--3654},
  month = {May},
  doi = {10.1109/ICASSP.1995.479778},
  file = {Marzetta_1995.pdf:Marzetta_1995.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Mazur2007,
  author = {R. Mazur and A. Mertins},
  title = {Solving the Permutation Problem in Convolutive Blind Source Separation},
  booktitle = {ICA},
  year = {2007},
  file = {Mazur_and_Mertins_2007.pdf:Mazur_and_Mertins_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.08}
}

@BOOK{McLachlan1997,
  title = {The EM Algorithm and Extensions},
  publisher = {Wiley, New York, USA},
  year = {1997},
  author = {G. McLachlan and T. Krishnan},
  owner = {ozerov},
  timestamp = {2008.12.09}
}

@INPROCEEDINGS{Meron1998,
  author = {Y. Meron and K. Hirose},
  title = {Separation of singing and piano sounds},
  booktitle = {Proc. Int. Conf. on Spoken Language Processing},
  year = {1998},
  owner = {ozerov},
  timestamp = {2011.06.08}
}

@INPROCEEDINGS{Mesaros2007,
  author = {Mesaros, A. and Virtanen, T. and Klapuri, A.},
  title = {Singer Identification in Polyphonic Music Using Vocal Separation
	and Pattern Recognition Methods},
  booktitle = {International Conference on Music Information Retrieval},
  year = {2007},
  address = {Vienna, Austria},
  owner = {ozerov},
  timestamp = {2007.12.14}
}

@INPROCEEDINGS{Meurisse2006,
  author = {G. Meurisse and P. Hanna and S. Marchand},
  title = {A New Analysis Method for Sinusoids+Noise Spectral Models},
  booktitle = {Proceedings of the Digital Audio Effects (DAFx06) Conference},
  year = {2006},
  pages = {139--144},
  month = {September},
  file = {Meurisse_et_al_2006.pdf:Meurisse_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Mokios2008,
  author = {Mokios, K.N. and Potamianos, A. and Sidiropoulos, N.D.},
  title = {On the effectiveness of PARAFAC-based estimation for blind speech
	separation},
  booktitle = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE
	International Conference on},
  year = {2008},
  pages = {153--156},
  month = {March 31 2008-April 4},
  abstract = {This work establishes the effectiveness of parallel factor (PARAFAC)
	analysis in blind speech separation (BSS) problems. The BSS problem
	is formulated as a conjugate-symmetric PARAFAC model that is fitted
	optimally, using an efficient alternating least-squares algorithm
	that converges monotonically. The identifiability properties of the
	model are also presented, revealing the much broader identifiability
	potential of joint-diagonalization- based BSS methods. In order to
	focus on estimation performance, perfect resolution of the permutation
	ambiguity is assumed. Simulations under varying reverberation conditions
	and comparison with previous estimation methods that are widely used
	in BSS problems demonstrate significant performance gains. Signal-to-
	interference (SIR) ratio improvement of over 27 dB is achieved using
	PARAFAC. Average SIR gains of 2.5 and 6.3 dB are achieved compared
	to state-of-the-art FastICA[2] and FDSOS (Parra's)[5] estimation
	algorithms, respectively.},
  doi = {10.1109/ICASSP.2008.4517569},
  file = {Mokios_et_al_2008.pdf:Mokios_et_al_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.19}
}

@INPROCEEDINGS{Moulines1997,
  author = {Moulines, E. and Cardoso, J.-F. and Gassiat, E.},
  title = {Maximum likelihood for blind separation and deconvolution of noisy
	signals using mixture models},
  booktitle = {Proc.~IEEE International Conference on Acoustics, Speech, and Signal
	Processing (ICASSP'97)},
  year = {1997},
  pages = {3617 -- 3620},
  month = {April},
  doi = {10.1109/ICASSP.1997.604649},
  file = {Moulines_et_al_1997.pdf:Moulines_et_al_1997.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.08}
}

@TECHREPORT{Murphy1998,
  author = {K. P. Murphy},
  title = {Learning Switching {K}alman Filter Models},
  institution = {Compaq Cambridge Research Lab},
  year = {1998},
  number = {98-10},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Nakatani2011,
  author = {T. Nakatani and S. Araki and T. Yoshioka and M. Fujimoto},
  title = {Joint unsupervised learning of hidden {M}arkov source models and
	source location models for multichannel source separation},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'11)},
  year = {2011},
  address = {Prague, Czech Republic},
  month = {May},
  owner = {ozerov},
  timestamp = {2011.06.10}
}

@ARTICLE{Neeser1993,
  author = {F. D. Neeser and J. L. Massey},
  title = {Proper complex random processes with applications to information
	theory},
  journal = {IEEE Transactions on Information Theory},
  year = {1993},
  volume = {39},
  pages = {1293--1302},
  number = {4},
  month = {July},
  file = {Neeser_and_Massey_1993.pdf:Neeser_and_Massey_1993.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.03.17}
}

@INPROCEEDINGS{Nesta2009,
  author = {F. Nesta and P. Svaizer and M. Omologo},
  title = {Cumulative State Coherence Transform for a Robust Two-Channel Multiple
	Source Localization},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'09)},
  year = {2009},
  pages = {290--297},
  owner = {ozerov},
  timestamp = {2010.04.12}
}

@INPROCEEDINGS{O'Grady2004,
  author = {P. D. O'Grady and P. A. Pearlmutter},
  title = {Soft-{LOST}: {EM} on a Mixture of Oriented Lines},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA)},
  year = {2004},
  pages = {428--435},
  file = {O_Grady_and_Pearlmutter_2004.pdf:O_Grady_and_Pearlmutter_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.18}
}

@ARTICLE{Oudot2001,
  author = {Oudot, M. and Cappe, O. and Moulines, E.},
  title = {Estimation of the spectral envelope of voiced sounds using a penalized
	likelihood approach},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {2001},
  volume = {9},
  pages = {469--481},
  number = {5},
  file = {Oudot_et_al_2001.pdf:Oudot_et_al_2001.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Ozerov2009,
  author = {A. Ozerov and C. F\'evotte},
  title = {Multichannel nonnegative matrix factorization in convolutive mixtures.
	With application to blind audio source separation},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'09)},
  year = {2009},
  owner = {ozerov},
  timestamp = {2008.12.04}
}

@INPROCEEDINGS{Ozerov2011,
  author = {A. Ozerov and C. F\'evotte and R. Blouet and J.-L. Durrieu},
  title = {Multichannel Nonnegative Tensor Factorization With Structured Constraints
	For User-Guided Audio Source Separation},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'11)},
  year = {2011},
  pages = {257 -- 260},
  address = {Prague, Czech Republic},
  month = {May},
  owner = {Aliosha},
  timestamp = {2010.10.27}
}

@ARTICLE{Ozerov2009b,
  author = {A. Ozerov and C. F{\'e}votte},
  title = {Multichannel nonnegative matrix factorization in convolutive mixtures
	for audio source separation},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2010},
  volume = {18},
  pages = {550--563},
  number = {3},
  month = {March},
  owner = {ozerov},
  timestamp = {2009.01.03}
}

@INPROCEEDINGS{Ozerov2009a,
  author = {Ozerov, A. and F{\'e}votte, C. and Charbit, M.},
  title = {Factorial Scaled Hidden {M}arkov Model for polyphonic audio representation
	and source separation},
  booktitle = {Proc. IEEE Workshop on Applications of Signal Processing to Audio
	and Acoustics (WASPAA '09)},
  year = {2009},
  pages = {121--124},
  month = oct # { 18--21,},
  doi = {10.1109/ASPAA.2009.5346527},
  owner = {Aliosha},
  timestamp = {2010.01.03}
}

@ARTICLE{Ozerov2007a,
  author = {A. Ozerov and R. Gribonval and P. Philippe and F. Bimbot},
  title = {Choix et adaptation des modèles pour la séparation de voix chantée
	à partir d'un seul microphone},
  journal = {Traitement du signal},
  year = {2007},
  volume = {24},
  pages = {211-224},
  number = {3},
  owner = {ozerov},
  timestamp = {2007.12.18}
}

@ARTICLE{Ozerov2007,
  author = {Ozerov, A. and Philippe, P. and Bimbot, F. and Gribonval, R.},
  title = {Adaptation of Bayesian Models for Single-Channel Source Separation
	and its Application to Voice/Music Separation in Popular Songs},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2007},
  volume = {15},
  pages = {1564--1578},
  number = {5},
  month = {July },
  doi = {10.1109/TASL.2007.899291},
  owner = {ozerov},
  timestamp = {2007.06.29}
}

@INPROCEEDINGS{Ozerov2011a,
  author = {A. Ozerov and E. Vincent},
  title = {Using the {FASST} source separation toolbox for noise robust speech
	recognition},
  booktitle = {International Workshop on Machine Listening in Multisource Environments
	(CHiME 2011)},
  year = {2011},
  pages = {86--87},
  address = {Florence, Italy},
  month = {September},
  owner = {ozerov},
  timestamp = {2011.06.08}
}

@MISC{FASST,
  author = {A. Ozerov and E. Vincent and F. Bimbot},
  title = {{F}lexible {A}udio {S}ource {S}eparation {T}oolbox ({FASST})},
  owner = {Aliosha},
  timestamp = {2011.05.21},
  url = {http://bass-db.gforge.inria.fr/fasst/}
}

@ARTICLE{Ozerov2010a,
  author = {A. Ozerov and E. Vincent and F. Bimbot},
  title = {A General Flexible Framework for the Handling of Prior Information
	in Audio Source Separation},
  journal = {IEEE Transactions on Audio, Speech and Signal Processing},
  volume = 20,
  number = 4,
  pages = {1118--1133},
  year = {2012}
}

@INPROCEEDINGS{Ozerov2010,
  author = {A. Ozerov and E. Vincent and F. Bimbot},
  title = {A general modular framework for audio source separation},
  booktitle = {9th International Conference on Latent Variable Analysis and Signal
	Separation (LVA/ICA'10)},
  year = {2010},
  pages = {33--40},
  address = {Saint-Malo, France},
  month = {Sep. 27-30},
  owner = {ozerov},
  timestamp = {2010.09.10}
}

@ARTICLE{Paatero1994,
  author = {P. Paatero and U. Tapper},
  title = {Positive matrix factorization: a non-negative factor model with optimal
	utilization of error estimates of data values},
  journal = {Environmetrics},
  year = {1994},
  volume = {5},
  pages = {111--126},
  owner = {Aliosha},
  timestamp = {2010.11.18}
}

@PHDTHESIS{Park2004,
  author = {Tae Hong Park},
  title = {Towards Automatic Musical Instrument Timbre Recognition},
  school = {Princeton University},
  year = {2004},
  address = {NJ, USA},
  month = {November},
  abstract = { This dissertation is comprised of two parts  focus on issues concerning
	research and development of an artificial system for automatic musical
	instrument timbre recognition and musical compositions. The technical
	part of the essay includes a detailed record of developed and implemented
	algorithms for feature extraction and pattern recognition. A review
	of existing literature introducing historical aspects surrounding
	timbre research, problems associated with a number of timbre definitions,
	and highlights of selected research activities that have had significant
	impact in this field are also included. The developed timbre recognition
	system follows a bottom-up, data-driven model that includes a preprocessing
	module, feature extraction module, and a RBF/EBF (Radial/Elliptical
	Basis Function) neural network-based pattern recognition module.
	829 monophonic samples from 12 instruments have been chosen from
	the Peter Siedlaczek library (Best Service) and other samples from
	the internet and personal collections. Significant emphasis has been
	put on feature extraction development and testing to achieve robust
	and consistent feature vectors that are eventually passed to the
	neural network module. In order to avoid a garbage-in-garbage-out
	(GIGO) trap and improve generality, extra care was taken in designing
	and testing the developed algorithms using various dynamics, different
	playing techniques, and a variety of pitches for each instrument
	with inclusion of attack and steady-state portions of a signal. Most
	of the research and development was conducted in Matlab. The compositional
	part of the essay includes brief introductions to A dEss Are,
	Aboji, 48 13 N, 16 20 O, and pH SQ. A general outline pertaining
	to the ideas and concepts behind the architectural designs of the
	pieces including formal structures, time structures, orchestration
	methods, and pitch structures are also presented.},
  file = {Park_PhD_2004.pdf:Park_PhD_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11},
  url = {http://www.tulane.edu/~park/publications/ParkPrincetonThesis2004.pdf}
}

@ARTICLE{Parra2000,
  author = {L. Parra and C. Spence},
  title = {Convolutive Blind Source Separation of Non-Stationary Sources},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {2000},
  volume = {8},
  pages = {320--327},
  number = {3},
  file = {Parra_and_Spence_2000.pdf:Parra_and_Spence_2000.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@ARTICLE{Pham2001,
  author = {D.-T.~Pham and J.-F.~Cardoso},
  title = {Blind separation of instantaneous mixtures of non stationary sources},
  journal = {IEEE Transactions on Signal Processing},
  year = {2001},
  volume = {49},
  pages = {1837--1848},
  number = {9},
  month = {Sep.},
  file = {Pham_and_Cardoso_2000.pdf:Pham_and_Cardoso_2000.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.09.23}
}

@INPROCEEDINGS{Pham2003,
  author = {D.-T. Pham and C. Servi{\`e}re and H. Boumaraf},
  title = {Blind separation of speech mixtures based on nonstationarity},
  booktitle = {Proceedings of the 7th International Symposium on Signal Processing
	and its Applications},
  year = {2003},
  pages = {II--73--76},
  owner = {ozerov},
  timestamp = {2010.04.02}
}

@ARTICLE{Rabiner1989,
  author = {Rabiner, L. R. },
  title = {A tutorial on hidden {M}arkov models and selected applications in
	speech recognition},
  journal = {Proceedings of the IEEE},
  year = {1989},
  volume = {77},
  pages = {257--286},
  number = {2},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov
	models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and
	gives practical details on methods of implementation of the theory
	along with a description of selected applications of the theory to
	distinct problems in speech recognition. Results from a number of
	original sources are combined to provide a single source of acquiring
	the background required to pursue further this area of research.
	The author first reviews the theory of discrete Markov chains and
	shows how the concept of hidden states, where the observation is
	a probabilistic function of the state, can be used effectively. The
	theory is illustrated with two simple examples, namely coin-tossing,
	and the classic balls-in-urns system. Three fundamental problems
	of HMMs are noted and several practical techniques for solving these
	problems are given. The various types of HMMs that have been studied,
	including ergodic as well as left-right models, are described},
  file = {Rabiner_1989.pdf:Rabiner_1989.pdf:PDF},
  owner = {ozerov},
  priority = {2},
  timestamp = {2009.03.13}
}

@INPROCEEDINGS{Radfar2006,
  author = {Radfar, M.H. and Dansereau, R.M. and Sayadiyan, A.},
  title = {A Joint Identification-Separation Technique for Single Channel Speech
	Separation},
  booktitle = {Digital Signal Processing Workshop, 12th - Signal Processing Education
	Workshop, 4th},
  year = {2006},
  pages = {76--81},
  month = {Sept.},
  abstract = {We present a generalized approach to speaker dependent model-based
	single channel speech separation techniques in which a priori knowledge
	of the underlying speakers is used to separate speech signals. For
	this purpose, we add an identification stage by which we first identify
	the underlying speakers in the mixture and then use the identified
	speakers' model to separate speech signals. The proposed technique
	not only preserves the advantages of model-based speaker dependent
	single channel speech separation algorithms (i.e. high separability)
	but also is able to separate the speech signals of an unlimited number
	of speakers given the speakers' models (i.e. generality). Evaluation
	results conducted on a database consisting of 100 mixed speech signals
	with target-to-interference ratios (TIR) ranging -9 dB to +9 dB show
	significant performance improvements over those techniques which
	use a single model for all speakers},
  doi = {10.1109/DSPWS.2006.265432},
  file = {Radfar_et_al_2006.pdf:Radfar_et_al_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@MISC{Rafii2007,
  author = {Z. Rafii and R. Blouet and A. Liutkus},
  title = {Discriminant Approach within Non-negative Matrix Factorization for
	Musical Components Recognition},
  howpublished = {DMRN+2: Digital Music Research Network One-day Workshop},
  month = {Dec.},
  year = {2007},
  note = {Poster presentation},
  file = {Rafii_2007.pdf:Rafii_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.07}
}

@INPROCEEDINGS{Rennie2008,
  author = {Rennie, S. J. and Hershey, J. R. and Olsen, P. A. },
  title = {Efficient model-based speech separation and denoising using non-negative
	subspace analysis},
  booktitle = {Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing (ICASSP'08)},
  year = {2008},
  pages = {1833--1836},
  abstract = {We present a new probabilistic architecture for <span class='snippet'>analyzing</span>
	composite <span class='snippet'>non</span>-<span class='snippet'>negative</span>
	data, called <span class='snippet'>Non</span>-<span class='snippet'>negative</span>
	<span class='snippet'>Subspace</span> <span class='snippet'>Analysis</span>
	(NSA). The NSA <span class='snippet'>model</span> provides a framework
	for understanding the relationships between sparse <span class='snippet'>subspace</span>
	<span class='snippet'>and</span> mixture <span class='snippet'>model</span>
	<span class='snippet'>based</span> approaches, <span class='snippet'>and</span>
	encompasses a range of models, including Sparse <span class='snippet'>Non</span>-<span
	class='snippet'>negative</span> Matrix Factorization (SNMF) [1] <span
	class='snippet'>and</span> mixture-<span class='snippet'>model</span>
	<span class='snippet'>based</span> <span class='snippet'>analysis</span>
	as special cases. We present a convenient instantiation of the NSA
	<span class='snippet'>model</span>, <span class='snippet'>and</span>
	an <span class='snippet'>efficient</span> variational approximate
	learning <span class='snippet'>and</span> inference algorithm that
	combines the advantages of SNMF <span class='snippet'>and</span>
	mixture <span class='snippet'>model</span>-<span class='snippet'>based</span>
	approaches. Preliminary recognition results on the Pascal <span class='snippet'>Speech</span>
	<span class='snippet'>Separation</span> Challenge 2006 test set [2],
	<span class='snippet'>based</span> on NSA <span class='snippet'>separation</span>
	results, are presented. The results fall short of those achieved
	by Algonquin [3], a state-of-the-art mixture-<span class='snippet'>model</span>
	<span class='snippet'>based</span> method, but considering that NSA
	runs an order of magnitude faster, the results are impressive. NSA
	outperforms SNMF in terms of word error rate (WER) on the task by
	a significant margin of over 9% absolute.},
  doi = {10.1109/ICASSP.2008.4517989},
  owner = {Aliosha},
  timestamp = {2010.10.19}
}

@BOOK{Rijsbergen1979,
  title = {Information Retrieval},
  publisher = {Butterworths, London},
  year = {1979},
  author = {C. J. van Rijsbergen},
  owner = {ozerov},
  timestamp = {2008.10.30}
}

@ARTICLE{Rivet2007,
  author = {B. Rivet and L. Girin and C. Jutten},
  title = {Mixing Audiovisual Speech Processing and Blind Source Separation
	for the Extraction of Speech Signals From Convolutive Mixtures},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2007},
  volume = {15},
  pages = {96--108},
  number = {1},
  abstract = {Looking at the speaker's face can be useful to better hear a <span
	class='snippet'>speech</span> signal in noisy environment <span class='snippet'>and</span>
	extract it from competing sources before identification. This suggests
	that the visual signals of <span class='snippet'>speech</span> (movements
	of visible articulators) could be used in <span class='snippet'>speech</span>
	enhancement or extraction systems. In this paper, we present a novel
	algorithm plugging <span class='snippet'>audiovisual</span> coherence
	of <span class='snippet'>speech</span> signals, estimated by statistical
	tools, on audio <span class='snippet'>blind</span> source separation
	(BSS) techniques. This algorithm is applied to the difficult <span
	class='snippet'>and</span> realistic case of convolutive mixtures.
	The algorithm mainly works in the frequency (transform) domain, where
	the convolutive mixture becomes an additive mixture for each frequency
	channel. Frequency by frequency separation is made by an audio BSS
	algorithm. The audio <span class='snippet'>and</span> visual informations
	are modeled by a newly proposed statistical model. This model is
	then used to solve the standard source permutation <span class='snippet'>and</span>
	scale factor ambiguities encountered for each frequency after the
	audio <span class='snippet'>blind</span> separation stage. The proposed
	method is shown to be efficient in the case of 2 times 2 convolutive
	mixtures <span class='snippet'>and</span> offers promising perspectives
	for extracting a particular <span class='snippet'>speech</span> source
	of interest from complex mixtures},
  doi = {10.1109/TASL.2006.872619},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Roweis2000,
  author = {S. T. Roweis},
  title = {One Microphone Source Separation},
  booktitle = {Advances in Neural Information Processing Systems 13},
  year = {2000},
  pages = {793--799},
  publisher = {MIT Press},
  owner = {Aliosha},
  timestamp = {2010.10.19}
}

@INPROCEEDINGS{Sawada2007,
  author = {Sawada, H. and Araki, S. and Makino, S.},
  title = {Measuring Dependence of Bin-wise Separated Signals for Permutation
	Alignment in Frequency-domain {BSS}},
  booktitle = {IEEE International Symposium on Circuits and Systems (ISCAS'07)},
  year = {2007},
  pages = {3247--3250},
  month = {27-30 May},
  abstract = {This paper presents a new method for grouping bin-wise separated signals
	for individual sources, i.e., solving the permutation problem, in
	the process of frequency-domain blind source separation. Conventionally,
	the correlation coefficient of separated signal envelopes is calculated
	to judge whether or not the separated signals originate from the
	same source. In this paper, we propose a new measure that represents
	the dominance of the separated signal in the mixtures, and use it
	for calculating the correlation coefficient, instead of a signal
	envelope. Such dominance measures exhibit dependence/independence
	more clearly than traditionally used signal envelopes. Consequently,
	a simple clustering algorithm with centroids works well for grouping
	separated signals. Experimental results were very appealing, as three
	sources including two coming from the same direction were separated
	properly with the new method.},
  doi = {10.1109/ISCAS.2007.378164},
  file = {Sawada_et_al_2007.pdf:Sawada_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.04}
}

@ARTICLE{Sawada2007a,
  author = {Sawada, H. and Araki, S. and Mukai, R. and Makino, S. },
  title = {Grouping Separated Frequency Components by Estimating Propagation
	Model Parameters in Frequency-Domain Blind Source Separation},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2007},
  volume = {15},
  pages = {1592--1604},
  number = {5},
  __markedentry = {[Aliosha]},
  abstract = {This paper proposes a new formulation and optimization procedure for
	<span class='snippet'>grouping</span> <span class='snippet'>frequency</span>
	<span class='snippet'>components</span> <span class='snippet'>in</span>
	<span class='snippet'>frequency</span>-domain blind source separation
	(BSS). We adopt two separation techniques, independent <span class='snippet'>component</span>
	analysis (ICA) and time-<span class='snippet'>frequency</span> (T-F)
	masking, for the <span class='snippet'>frequency</span>-domain BSS.
	<span class='snippet'>With</span> ICA, <span class='snippet'>grouping</span>
	the <span class='snippet'>frequency</span> <span class='snippet'>components</span>
	corresponds to aligning the permutation ambiguity of the ICA solution
	<span class='snippet'>in</span> each <span class='snippet'>frequency</span>
	bin. <span class='snippet'>With</span> T-F masking, <span class='snippet'>grouping</span>
	the <span class='snippet'>frequency</span> <span class='snippet'>components</span>
	corresponds to classifying sensor observations <span class='snippet'>in</span>
	the time-<span class='snippet'>frequency</span> domain for individual
	sources. The <span class='snippet'>grouping</span> procedure is based
	on <span class='snippet'>estimating</span> anechoic <span class='snippet'>propagation</span>
	<span class='snippet'>model</span> <span class='snippet'>parameters</span>
	by analyzing ICA results or sensor observations. More specifically,
	the time delays of arrival and attenuations from a source to all
	sensors are <span class='snippet'>estimated</span> for each source.
	The focus of this paper includes the applicability of the proposed
	procedure for a situation <span class='snippet'>with</span> wide
	sensor spacing where spatial aliasing may occur. Experimental results
	show that the proposed procedure effectively <span class='snippet'>separates</span>
	two or three sources <span class='snippet'>with</span> several sensor
	configurations <span class='snippet'>in</span> a real room, as long
	as the room reverberation is moderately low.},
  doi = {10.1109/TASL.2007.899218},
  owner = {Aliosha},
  timestamp = {2010.11.06}
}

@INPROCEEDINGS{Sharman1988,
  author = {Sharman, K.C.},
  title = {Maximum likelihood parameter estimation by simulated annealing},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal
	Processing (ICASSP'88)},
  year = {1988},
  pages = {2741--2744},
  month = {11-14 April},
  abstract = {The author presents a method for finding the maximum-likelihood estimates
	of a set of signal parameters. The algorithm is based on simulated
	annealing, which is a form of stochastic optimization that has been
	found to be a powerful technique for solving multidimensional combinatorial
	optimization problems. He also presents a simulated annealing solution
	to a typical parameter estimation problem that arises in sensor array
	processing, and some experimental results are included which demonstrate
	the power of annealing compared to existing algorithms. It is pointed
	out that simulated annealing is quite a general optimization procedure
	and should find a wide range of applications},
  doi = {10.1109/ICASSP.1988.197217},
  file = {Sharman_1988.pdf:Sharman_1988.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.19}
}

@INPROCEEDINGS{Slezak2002,
  author = {Dominik Slezak and Piotr Synak and Alicja Wieczorkowska and Jakub
	Wroblewski},
  title = {KDD-Based Approach to Musical Instrument Sound Recognition},
  booktitle = {ISMIS},
  year = {2002},
  pages = {28-36},
  file = {Slezak_et_al_2002.ps:Slezak_et_al_2002.ps:PDF},
  owner = {ozerov},
  timestamp = {2008.02.11}
}

@INPROCEEDINGS{Smaragdis2004,
  author = {P. Smaragdis},
  title = {Non-negative matrix factor deconvolution; extraction of multiple
	sound sources from monophonic inputs.},
  booktitle = {Fifth International Conference on Independent Component Analysis},
  year = {2004},
  pages = {494--499},
  address = {Granada, Spain},
  month = {Sep.},
  file = {Smaragdis_2004.pdf:Smaragdis_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.07.07}
}

@INPROCEEDINGS{Smaragdis2003,
  author = {Smaragdis, P. and Brown, J.C.},
  title = {Non-negative matrix factorization for polyphonic music transcription},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2003 IEEE
	Workshop on.},
  year = {2003},
  pages = {177--180},
  month = {19-22 Oct.},
  abstract = {We present a methodology for analyzing polyphonic musical passages
	comprised of notes that exhibit a harmonically fixed spectral profile
	(such as piano notes). Taking advantage of this unique note structure,
	we can model the audio content of the musical passage by a linear
	basis transform and use non-negative matrix decomposition methods
	to estimate the spectral profile and the temporal information of
	every note. This approach results in a very simple and compact system
	that is not knowledge-based, but rather learns notes by observation.},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@INPROCEEDINGS{Smaragdis2009,
  author = {Smaragdis, P. and Mysore, G. J. },
  title = {Separation by "humming": {U}ser-guided sound extraction from monophonic
	mixtures},
  booktitle = {Proceedings IEEE Workshop Applications of Signal Processing to Audio
	and Acoustics (WASPAA '09)},
  year = {2009},
  pages = {69--72},
  abstract = {In this paper we present a novel approach for isolating and removing
	<span class='snippet'>sounds</span> <span class='snippet'>from</span>
	dense <span class='snippet'>monophonic</span> <span class='snippet'>mixtures</span>.
	The approach is <span class='snippet'>user</span>-based, and requires
	the presentation of a guide <span class='snippet'>sound</span> that
	mimics the desired target the <span class='snippet'>user</span> wishes
	to extract. The guide <span class='snippet'>sound</span> can be simply
	produced <span class='snippet'>from</span> a <span class='snippet'>user</span>
	by vocalizing or otherwise replicating the target <span class='snippet'>sound</span>
	marked for separation. Using that guide as a prior in a statistical
	<span class='snippet'>sound</span> <span class='snippet'>mixtures</span>
	model, we propose a methodology that allows us to efficiently extract
	complex structured <span class='snippet'>sounds</span> <span class='snippet'>from</span>
	dense <span class='snippet'>mixtures</span>.},
  doi = {10.1109/ASPAA.2009.5346542},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@INPROCEEDINGS{Spiertz2009,
  author = {M. Spiertz and V. Gnann},
  title = {Source-filter based clustering for monaural blind source separation},
  booktitle = {Proceedings of International Conference on Digital Audio Effects
	(DAFx'09)},
  year = {2009},
  address = {Como, Italy},
  month = {Sept.},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@ARTICLE{Talukdar1991,
  author = {K. K. Talukdar and W. D. Lawing},
  title = {Estimation of the parameters of the {R}ice distribution},
  journal = {Journal of the Acoustical Society of America},
  year = {1991},
  volume = {89},
  pages = {1193--1197},
  number = {3},
  month = {March},
  file = {Talukdar_and_Lawing_1991.pdf:Talukdar_and_Lawing_1991.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{Vincent2007b,
  author = {E. Vincent},
  title = {Complex nonconvex lp norm minimization for underdetermined source
	separation},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'07)},
  year = {2007},
  pages = {430--437},
  file = {Vincent_2007.pdf:Vincent_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.05}
}

@ARTICLE{Vincent2006,
  author = {E. Vincent},
  title = {Musical source separation using time-frequency source priors},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2006},
  volume = {14},
  pages = {91--98},
  number = {1},
  file = {Vincent_2006.pdf:Vincent_2006.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.03.20}
}

@INPROCEEDINGS{Vincent2009a,
  author = {E. Vincent and S. Araki and P. Bofilld},
  title = {The 2008 Signal Separation Evaluation Campaign: A community-based
	approach to large-scale evaluation},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Signal Separation
	(ICA'09)},
  year = {2009},
  pages = {734-741},
  file = {Vincent_et_al_2009a.pdf:Vincent_et_al_2009a.pdf:PDF},
  owner = {ozerov},
  timestamp = {2009.01.03}
}

@INPROCEEDINGS{Vincent2009,
  author = {E. Vincent and S. Arberet and R. Gribonval},
  title = {Underdetermined Instantaneous Audio Source Separation via Local {G}aussian
	Modeling},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'09)},
  year = {2009},
  pages = {775 -- 782},
  owner = {ozerov},
  timestamp = {2008.12.20}
}

@ARTICLE{Vincent2010,
  author = {E Vincent and N Bertin and R Badeau},
  title = {Adaptive harmonic spectral decomposition for multiple pitch estimation},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2010},
  volume = {18},
  pages = {528--537},
  number = {3},
  owner = {ozerov},
  timestamp = {2010.03.22}
}

@INPROCEEDINGS{Vincent2007a,
  author = {E. Vincent and N. Bertin and R. Badeau},
  title = {Two nonnegative matrix factorization methods for polyphonic pitch
	transcription},
  booktitle = {Proc. Music Information Retrieval Evaluation eXchange (MIREX)},
  year = {2007},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@ARTICLE{Vincent2006a,
  author = {Vincent, E. and Gribonval, R. and Fevotte, C. },
  title = {Performance measurement in blind audio source separation},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2006},
  volume = {14},
  pages = {1462--1469},
  number = {4},
  month = jul,
  doi = {10.1109/TSA.2005.858005},
  owner = {ozerov},
  timestamp = {2010.01.05}
}

@INCOLLECTION{Vincent2010a,
  author = {E Vincent and M.G. Jafari and S. A. Abdallah and M. D. Plumbley and
	M. E. Davies},
  title = {Probabilistic modeling paradigms for audio source separation},
  booktitle = {Machine Audition: Principles, Algorithms and Systems},
  publisher = {IGI Global},
  year = {2010},
  chapter = {7},
  pages = {162--185},
  owner = {Aliosha},
  timestamp = {2010.04.05}
}

@INPROCEEDINGS{Vincent2007,
  author = {E. Vincent and H. Sawada and P. Bofill and S. Makino and J. P. Rosca},
  title = {First Stereo Audio Source Separation Evaluation Campaign: Data, Algorithms
	and Results.},
  booktitle = {Proc. Int. Conf. on Independent Component Analysis and Blind Source
	Separation (ICA'07)},
  year = {2007},
  pages = {552-559},
  publisher = {Springer},
  file = {SASSEC_ICA07.pdf:SASSEC_ICA07.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.09.24}
}

@INPROCEEDINGS{Vinyes2006,
  author = {Vinyes, M. and Bonada, J. and Loscos, A.},
  title = {Demixing Commercial Music Productions via Human-Assisted Time-Frequency
	Masking},
  booktitle = {Proceedings of Audio Engineering Society 120th Convention},
  year = {2006},
  owner = {Aliosha},
  timestamp = {2010.10.07}
}

@ARTICLE{Virtanen2007,
  author = {T. Virtanen},
  title = {Monaural sound source separation by non-negative matrix factorization
	with temporal continuity and sparseness criteria},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2007},
  volume = {15},
  pages = {1066--1074},
  number = {3},
  owner = {ozerov},
  timestamp = {2008.11.06}
}

@INPROCEEDINGS{Walsh2007,
  author = {John MacLaren Walsh and Youngmoo E. Kim and Travis M. Doll},
  title = {Joint iterative multi-speaker identification and source separation
	using expectation propagation},
  booktitle = {Applications of Signal Processing to Audio and Acoustics, 2007 IEEE
	Workshop on},
  year = {2007},
  month = {21-24 Oct.},
  file = {Walsh_et_al_2007.pdf:Walsh_et_al_2007.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.01.02}
}

@INPROCEEDINGS{Wang2011,
  author = {Y. Wang and Z. Ou},
  title = {Combining {HMM}-based melody extraction and {NMF}-based soft masking
	for separating voice and accompaniment from monaural audio},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP'11)},
  year = {2011},
  address = {Prague, Czech Republic},
  month = {May},
  owner = {ozerov},
  timestamp = {2011.06.10}
}

@ARTICLE{Weiss2010,
  author = {R. Weiss and D. Ellis},
  title = {Speech separation using speaker-adapted eigenvoice speech models},
  journal = {Computer Speech and Language},
  year = {2010},
  volume = {24},
  pages = {16--29},
  number = {1},
  owner = {Aliosha},
  timestamp = {2010.10.18}
}

@INPROCEEDINGS{Weiss2008,
  author = {R. J. Weiss and M. I. Mandel and D. P. W. Ellis},
  title = {Source separation based on binaural cues and source model constraints},
  booktitle = {Interspeech'08},
  year = {2008},
  file = {Weiss_et_al_2008.pdf:Weiss_et_al_2008.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.11.04}
}

@INPROCEEDINGS{Winter2004,
  author = {S. Winter and H. Sawada and S. Araki and S. Makino},
  title = {Hierarchical clustering applied to overcomplete {BSS} for convolutive
	mixtures},
  booktitle = {ISCA Tutorial and Research Workshop on Statistical and Perceptual
	Audio Processing (SAPA 2004)},
  year = {2004},
  month = {Oct.},
  file = {Winter_et_al_2004.pdf:Winter_et_al_2004.pdf:PDF},
  owner = {ozerov},
  timestamp = {2008.12.15}
}

@TECHREPORT{Woodbury1950,
  author = {Max A. Woodbury},
  title = {Inverting modified matrices},
  institution = {Statistical Research Group, Princeton University},
  year = {1950},
  address = {Princeton, NJ},
  note = {Memo. Rept. 42},
  owner = {ozerov},
  timestamp = {2008.12.09}
}

@ARTICLE{Yilmaz2004,
  author = {Yilmaz, O. and Rickard, S.},
  title = {Blind separation of speech mixtures via time-frequency masking},
  journal = {IEEE Transactions on Signal Processing},
  year = {2004},
  volume = {52},
  pages = {1830--1847},
  number = {7},
  abstract = {Binary time-frequency masks are powerful tools for the <span class='snippet'>separation</span>
	<span class='snippet'>of</span> sources from a single <span class='snippet'>mixture</span>.
	Perfect demixing <span class='snippet'>via</span> binary time-frequency
	masks is possible provided the time-frequency representations <span
	class='snippet'>of</span> the sources do not overlap: a condition
	we call W-disjoint orthogonality. We introduce here the concept <span
	class='snippet'>of</span> approximate W-disjoint orthogonality and
	present experimental results demonstrating the level <span class='snippet'>of</span>
	approximate W-disjoint orthogonality <span class='snippet'>of</span>
	<span class='snippet'>speech</span> in <span class='snippet'>mixtures</span>
	<span class='snippet'>of</span> various orders. The results demonstrate
	that there exist ideal binary time-frequency masks that can separate
	several <span class='snippet'>speech</span> signals from one <span
	class='snippet'>mixture</span>. While determining these masks blindly
	from just one <span class='snippet'>mixture</span> is an open problem,
	we show that we can approximate the ideal masks in the case where
	two anechoic <span class='snippet'>mixtures</span> are provided.
	Motivated by the maximum likelihood mixing parameter estimators,
	we define a power weighted two-dimensional (2-D) histogram constructed
	from the ratio <span class='snippet'>of</span> the time-frequency
	representations <span class='snippet'>of</span> the <span class='snippet'>mixtures</span>
	that is shown to have one peak for each source with peak location
	corresponding to the relative attenuation and delay mixing parameters.
	The histogram is used to create time-frequency masks that partition
	one <span class='snippet'>of</span> the <span class='snippet'>mixtures</span>
	into the original sources. Experimental results on <span class='snippet'>speech</span>
	<span class='snippet'>mixtures</span> verify the technique. Example
	demixing results can be found online at http://alum.mit.edu/www/rickard/bss.html.},
  doi = {10.1109/TSP.2004.828896},
  owner = {Aliosha},
  timestamp = {2010.10.12}
}

@ARTICLE{Yoshioka2010,
  author = {Yoshioka, T and Nakatani, T and Miyoshi, M and Okuno, H},
  title = {Blind Separation and Dereverberation of Speech Mixtures by Joint
	Optimization},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2010},
  volume = {19},
  pages = {69 -- 84},
  number = {1},
  abstract = {This paper proposes a method for performing <span class='snippet'>blind</span>
	source <span class='snippet'>separation</span> (BSS) <span class='snippet'>and</span>
	<span class='snippet'>blind</span> <span class='snippet'>dereverberation</span>
	(BD) at the same time for <span class='snippet'>speech</span> <span
	class='snippet'>mixtures</span>. In most previous studies, BSS <span
	class='snippet'>and</span> BD have been investigated separately.
	The <span class='snippet'>separation</span> performance <span class='snippet'>of</span>
	conventional BSS methods deteriorates as the reverberation time increases
	while many existing BD methods rely on the assumption that there
	is only one sound source in a room. Therefore, it has been difcult
	to perform both BSS <span class='snippet'>and</span> BD when the
	reverberation time is long. The proposed method uses a network, in
	which <span class='snippet'>dereverberation</span> <span class='snippet'>and</span>
	<span class='snippet'>separation</span> networks are connected in
	tandem, to estimate source signals. The parameters for the <span
	class='snippet'>dereverberation</span> network (prediction matrices)
	<span class='snippet'>and</span> those for the <span class='snippet'>separation</span>
	network (<span class='snippet'>separation</span> matrices) are jointly
	optimized. This enables a BD process to take a BSS process into account.
	The prediction <span class='snippet'>and</span> <span class='snippet'>separation</span>
	matrices are alternately optimized with each depending on the other,
	hence we call the proposed method the conditional <span class='snippet'>separation</span>
	<span class='snippet'>and</span> <span class='snippet'>dereverberation</span>
	(CSD) method. Comprehensive evaluation results are reported, where
	all the <span class='snippet'>speech</span> materials contained in
	the complete test set <span class='snippet'>of</span> the TIMIT corpus
	are used. The CSD method improves the signal-to-interference ratio
	<span class='snippet'>by</span> an average <span class='snippet'>of</span>
	about 4 dB over the conventional frequency-domain BSS approach for
	reverberation times <span class='snippet'>of</span> 0.3 <span class='snippet'>and</span>
	0.5 sec. The direct-to-reverberation ratio is also improved <span
	class='snippet'>by</span> about 10 dB.},
  doi = {10.1109/TASL.2010.2045183},
  owner = {ozerov},
  timestamp = {2010.09.09}
}

@comment{jabref-meta: pdfDirectory:H:\\Work\\my_articles\\biblio;}

